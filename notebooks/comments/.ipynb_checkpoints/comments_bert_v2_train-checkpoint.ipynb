{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249dac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd() + '/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e7cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scripts.bert_utils import *\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4a7cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d'aww ! he match this background colour i be s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man , i be really not try to edit war . it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>`` more i can not make any real suggestion on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you , sir , be my hero . any chance you rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                       comment_text\n",
       "0      0  explanation why the edits make under my userna...\n",
       "1      0  d'aww ! he match this background colour i be s...\n",
       "2      0  hey man , i be really not try to edit war . it...\n",
       "3      0  `` more i can not make any real suggestion on ...\n",
       "4      0  you , sir , be my hero . any chance you rememb..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('data/comments/preprocessed_comments.csv', index_col=0)\n",
    "comments = comments.dropna()\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb241615",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text, test_text, temp_labels, test_labels = train_test_split(comments['comment_text'], \n",
    "                                                                  comments['toxic'], \n",
    "                                                                  random_state=0, \n",
    "                                                                  test_size=0.2, \n",
    "                                                                  stratify=comments['toxic'])\n",
    "\n",
    "\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(temp_text, \n",
    "                                                                  temp_labels, \n",
    "                                                                  random_state=0, \n",
    "                                                                  test_size=0.2, \n",
    "                                                                  stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e82a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1886b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1000.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "plt.hist(seq_len, bins=100)\n",
    "plt.vlines(256, 0, 60000, linestyles='dotted', color='k')\n",
    "plt.xlim(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e011cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 128\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = n_tokens,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = n_tokens,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = n_tokens,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb1d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a107d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b072dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b986b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b97e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58589896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f308b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.55300208 5.21679608]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', y=train_labels, classes=np.unique(train_labels))\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4141d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7e8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(model, train_dataloader, loss_function):\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_function(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b5f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "            \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "      \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b4863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.554\n",
      "Validation Loss: 0.452\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.407\n",
      "Validation Loss: 0.383\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.356\n",
      "Validation Loss: 0.336\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.337\n",
      "Validation Loss: 0.321\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.327\n",
      "Validation Loss: 0.312\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.320\n",
      "Validation Loss: 0.325\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.316\n",
      "Validation Loss: 0.301\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.313\n",
      "Validation Loss: 0.300\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.309\n",
      "Validation Loss: 0.302\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "  Batch   800  of  3,192.\n",
      "  Batch   900  of  3,192.\n",
      "  Batch 1,000  of  3,192.\n",
      "  Batch 1,100  of  3,192.\n",
      "  Batch 1,200  of  3,192.\n",
      "  Batch 1,300  of  3,192.\n",
      "  Batch 1,400  of  3,192.\n",
      "  Batch 1,500  of  3,192.\n",
      "  Batch 1,600  of  3,192.\n",
      "  Batch 1,700  of  3,192.\n",
      "  Batch 1,800  of  3,192.\n",
      "  Batch 1,900  of  3,192.\n",
      "  Batch 2,000  of  3,192.\n",
      "  Batch 2,100  of  3,192.\n",
      "  Batch 2,200  of  3,192.\n",
      "  Batch 2,300  of  3,192.\n",
      "  Batch 2,400  of  3,192.\n",
      "  Batch 2,500  of  3,192.\n",
      "  Batch 2,600  of  3,192.\n",
      "  Batch 2,700  of  3,192.\n",
      "  Batch 2,800  of  3,192.\n",
      "  Batch 2,900  of  3,192.\n",
      "  Batch 3,000  of  3,192.\n",
      "  Batch 3,100  of  3,192.\n",
      "\n",
      "Evaluating...\n",
      "  Batch   100  of  3,192.\n",
      "  Batch   200  of  3,192.\n",
      "  Batch   300  of  3,192.\n",
      "  Batch   400  of  3,192.\n",
      "  Batch   500  of  3,192.\n",
      "  Batch   600  of  3,192.\n",
      "  Batch   700  of  3,192.\n",
      "\n",
      "Training Loss: 0.306\n",
      "Validation Loss: 0.292\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train(model, train_dataloader, cross_entropy)\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/bert_v1.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30704a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'models/bert_v1.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb312e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 11.69 GiB (GPU 0; 6.00 GiB total capacity; 504.67 MiB already allocated; 4.13 GiB free; 568.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a54dd8d26970>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get predictions for test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-eb89afcd6563>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sent_id, mask)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#pass the inputs to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_hs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers-4.8.1-py3.8.egg\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 984\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    985\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers-4.8.1-py3.8.egg\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1916\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 11.69 GiB (GPU 0; 6.00 GiB total capacity; 504.67 MiB already allocated; 4.13 GiB free; 568.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ee3ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BERT, frozen parameters')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA/BElEQVR4nO3deXxU1fn48c+TnSwkQBZ2EhYhoOyLiIhIUFxxQcW6gbXWvfpVq/ZX6/LVr23V1tq6tpVq646yuFQFRFFBgSAgBJDFQMKWsCQkgZDt+f1xb2CIEzJZJpPleb9e88rMvffc+8wo88w5555zRFUxxhhjqgoKdADGGGOaJksQxhhjvLIEYYwxxitLEMYYY7yyBGGMMcYrSxDGGGO8sgRhWiQRSRKRRSJSICJPBToeY5ojSxCmXkQkU0QOiUihiOwXkQ9FpJvH/n+JSIm7v/Kxyt2XLCLqsT1TRO5z96312F4uIsUer3/jQ2g3AHuAtqp6l1/efCvm/nd9NNBxGP+yBGEawvmqGg10AnYDf62y/4+qGu3xGFRlf5xbfgrwgIhMVNUBlccDXwK3epT/Px9i6gFkaDUjQUUkpFbvsIlrbu+nucXbWlmCMA1GVYuBmUD/OpZfDqwFBtcnDhH5F3At8Gu3xpEmIg+JyEwR+Y+IHACmiUhnEZkrIvtEZJOI/MLjHHkeNZYit6aT7O47T0RWuscsFpGBHuUyReRuEVktIvki8paIRFQT5zQR+VpE/uoeu15EJnjsny4i69xmsi0i8kuPfaeLSLaI3Csiu4AZItJORD4QkVy3NveBiHT1KPO5iDzqxlwoIu+LSAcReU1EDojIssr36B7fT0TmuZ/PBhG5zN1+A3Clx+f7vru9s4i8617/RxG53eNc3j7/kSKy3L32bhH5U93+ixu/UVV72KPODyATSHOfRwKvAK967P8X8Gg1ZZMBBULc1ycDB4GLqhz3OXB9LeM65rrAQ0ApcCHOD6M2wBfAc0AETlLKBSZ4Odf/AYuAUGAokAOMAoJxElEmEO7xeSwFOgPtgXXAjdXEOA0oA+50z305kA+0d/efC/QCBBjnfjZD3X2nu2X/AIS776cDcIn73yEGeAeYXeVz3OSeMxbIAH4A0oAQ4FVghntsFJAFTHf3DcVpshtQzecbBKQDvwPCgJ7AFuCs43z+S4Cr3f3RwMmB/v/ZHsc+rAZhGsJsEckDDgATgSeq7L/b/bVd+Xilyv49InII5wvjOWC2n+JcoqqzVbUCiAdOBe5V1WJVXQn8A7jas4CIXA78DLhEVUuBXwAvquq3qlquqq8Ah3GSW6VnVHWHqu4D3uf4NaIc4GlVLVXVt4ANOIkBVf1QVTer4wvgU2CsR9kK4EFVPayqh1R1r6q+q6oHVbUAeAwnsXia4Z4zH/gvsFlV56tqGU5CGeIedx6QqaozVLVMVVcA7+I0A3ozAkhQ1UdUtURVtwB/B6Z6HHPk81fVQzgJo7eIxKtqoap+c5zPyQSAJQjTEC5U1TicX7K3Al+ISEeP/U+qapzH49oq5eNxfkHejfPLONRPcWZ5PO8M7HO/SCttBbpUvhCRIcDfcGo0ue7mHsBdngkP6Oaer9Iuj+cHcd5bdbarqmc/ydbKc4nI2SLyjdvEkwecg/NZVcpVp1mvMt5IEXlRRLa6zTiLgDgRCfYos9vj+SEvrytj7QGMqvI+rwQ8/7t66gF0rnL8b4Akj2OyqpT5OXACsN5t3jqvmnObALEEYRqM+4v6PaAc59d5bcs+BRQDN/sjPpzmrEo7gPYiEuOxrTuwHUBEEoBZOJ3j33kckwU8ViXhRarqG3WMqYuISJUYdohIOM4v9ieBJDcBf4TT3OTt/QDcBfQFRqlqW+A0d7tQe1nAF1XeZ7Sq3lTNtbOAH6scH6Oq51QXr6puVNUrgEScprKZIhJVh1iNn1iCMA1GHJOBdjht73Xxe5zOT68dux7XqrxFNrkuF1HVLGAx8LiIRLgdzT8HXhPnDpt3gdfcZh9PfwduFJFR7vuNEpFzqySa2kgEbheRUBG5FEjFSQRhODWyXKBMRM4GzqzhXDE4tYA8EWkPPFjHmAA+AE4Qkavd2EJFZISIpLr7d+P0M1RaChxwO83biEiwiJwoIiOqu4CIXCUiCW6TX567ubweMZsGZgnCNIT3RaQQpw/iMeBaVV3rsb/ybpfKx57jnOtDYD9OW//xdMNpjtlej7ivwOko34FTW3hQVecBXXHa+u+oEnd3de60+gVO09N+nE7fafWI4VugD04H8GPAFLcvoQC4HXjbvc7PgLk1nOtpnM7fPcA3wMd1Dcq9/pk4fQg7cJrNKjvEAf4J9Hebk2arajlwPk5/y49uDP/A6QyvziRgrfv/zl+AqZ5NZibw5NjmT2OaBxH5LU4b/IuBjqWuRGQazt1ZtWqOM6ax2GAV0yypqo3iNcbPrInJGGOMV9bEZIwxxiurQRhjjPGqRfVBxMfHa3JycqDDMMaYZiM9PX2PqiZ429eiEkRycjLLly8PdBjGGNNsiMjW6vZZE5MxxhivLEEYY4zxyhKEMcYYr1pUH4QxpmUpLS0lOzub4mKbgaO+IiIi6Nq1K6Ghvk+WbAnCGNNkZWdnExMTQ3JyMsdOemtqQ1XZu3cv2dnZpKSk+FzOmpiMMU1WcXExHTp0sORQTyJChw4dal0TswRhjGnSLDk0jLp8jq0+QRSXlvPSos18vel4M1AbY0zr49cEISKTRGSDiGwSkfu87D9dRPJFZKX7+J3HvkwR+d7d7rfRb2HBQby0aAtvLqu6GqIxxnh3zjnnkJeXd9xjoqO9rzQ7bdo0Zs6c6YeoGp7fOqnddXCfxVnEPhtYJiJzVTWjyqFfqmp1a9GOV1W//rQPChIm9EviozU7KSmrICyk1VeqjDHVUFVUlY8++ijQoTQKf34bjgQ2qeoWVS0B3gQm+/F6dZbWP4mC4jKWZe4LdCjGmEZw77338txzzx15/dBDD/Hwww8zYcIEhg4dykknncScOXMAyMzMJDU1lZtvvpmhQ4eSlZVFcnIye/Y4v10vvPBChg0bxoABA3jppZeOuc5dd93F0KFDmTBhArm5uT+JIz09nXHjxjFs2DDOOussdu7c6cd3XXv+TBBdcBYyr5TtbqtqtIisEpH/isgAj+0KfCoi6SJyQ3UXEZEbRGS5iCz39h/AF6f2jic8JIh5GbvrVN4Y07xMnTqVt946utz422+/zfTp05k1axYrVqxg4cKF3HXXXVQuh7BhwwauueYavvvuO3r06HHMuV5++WXS09NZvnw5zzzzDHv37gWgqKiIoUOHsmLFCsaNG8fDDz98TLnS0lJuu+02Zs6cSXp6Otdddx3/7//9Pz+/89rx5zgIb13mVRefWAH0UNVCETkHmI2zPi/AGFXdISKJwDwRWa+qi35yQtWXgJcAhg8fXqfFLdqEBXNq73jmr9vNg+f3t7smjGnhhgwZQk5ODjt27CA3N5d27drRqVMn7rzzThYtWkRQUBDbt29n927nR2OPHj04+eSTvZ7rmWeeYdasWQBkZWWxceNGOnToQFBQEJdffjkAV111FRdffPEx5TZs2MCaNWuYOHEiAOXl5XTq1Mlfb7lO/JkgsnEWlq/UFWfx8yNU9YDH849E5DkRiVfVPaq6w92eIyKzcJqsfpIgGkpa/yQWrM/hh92F9O0Y46/LGGOaiClTpjBz5kx27drF1KlTee2118jNzSU9PZ3Q0FCSk5OPjBuIioryeo7PP/+c+fPns2TJEiIjIzn99NOrHWtQ9YenqjJgwACWLFnSsG+sAfmziWkZ0EdEUkQkDJgKzPU8QEQ6ivupichIN569IhIlIjHu9ijgTGCNH2NlQr9EAOavs2YmY1qDqVOn8uabbzJz5kymTJlCfn4+iYmJhIaGsnDhQrZurXYW7CPy8/Np164dkZGRrF+/nm+++ebIvoqKiiN3K73++uuceuqpx5Tt27cvubm5RxJEaWkpa9eubcB3WH9+q0GoapmI3Ap8AgQDL6vqWhG50d3/AjAFuElEyoBDwFRVVRFJAma5uSMEeF1VP/ZXrACJbSMY1C2OeRm7uWV8b39eyhjTBAwYMICCggK6dOlCp06duPLKKzn//PMZPnw4gwcPpl+/fjWeY9KkSbzwwgsMHDiQvn37HtMMFRUVxdq1axk2bBixsbHH9HkAhIWFMXPmTG6//Xby8/MpKyvjjjvuYMCAAVUvEzAtak3q4cOHa30WDPrbZxt58tMfWPr/JpAYE9GAkRlj6mLdunWkpqYGOowWw9vnKSLpqjrc2/F207+HCalJAHy2LifAkRhjTOBZgvDQr2MMXeLaMN8ShDHGWILwJCJM7J/EV5tyOVRSHuhwjDEmoCxBVJGWmkRxaYVN3meMafUsQVQxMqU9MeEhdrurMabVswRRRVhIEKf1TWD+uhwqKlrOHV7GGFNbliC8mJiaxJ7Cw6zKzgt0KMYYEzCWILw4vW8CwUHCArubyZhWLS8v75hZX33ly3oR3jS1tSIsQXgRFxnGiOR21g9hTCtXXYIoLz/+XY4fffQRcXFxfoqq8fhzsr5mLS01iUc/XEfWvoN0ax8Z6HCMafUefn8tGTsO1HxgLfTv3JYHz69+aov77ruPzZs3M3jwYEJDQ4mOjqZTp06sXLmSjIwMLrzwQrKysiguLuZXv/oVN9zgrEyQnJzM8uXLKSws5Oyzz+bUU09l8eLFdOnShTlz5tCmTZsaY1uwYAF33303ZWVljBgxgueff57w8HDuu+8+5s6dS0hICGeeeSZPPvkk77zzDg8//DDBwcHExsayaFHDzGtqNYhqTOzvjKq2WoQxrdfvf/97evXqxcqVK3niiSdYunQpjz32GBkZzsKY1a0F4Wnjxo3ccsstrF27lri4ON59990ar1tcXMy0adN46623+P777ykrK+P5559n3759zJo1i7Vr17J69Wp++9vfAvDII4/wySefsGrVKubOnVvD2X1nNYhq9OgQRe/EaOav2830MSmBDseYVu94v/Qby8iRI0lJOfp9UN1aEJ5SUlIYPHgwAMOGDSMzM7PG62zYsIGUlBROOOEEAK699lqeffZZbr31ViIiIrj++us599xzOe88Z7XmMWPGMG3aNC677LKfrDtRH1aDOI601CS+3bKPA8WlgQ7FGNMEeK4L4bkWxKpVqxgyZIjXtSDCw8OPPA8ODqasrKzG61Q3iWpISAhLly7lkksuYfbs2UyaNAmAF154gUcffZSsrCwGDx7stSZTF5YgjmNi/0TKKpQvNtRtKVNjTPMWExNDQUGB133HWwuivvr160dmZiabNm0C4N///jfjxo2jsLCQ/Px8zjnnHJ5++mlWrlwJwObNmxk1ahSPPPII8fHxZGVlHefsvrMmpuMY3K0dHaLCmL9uN+cP6hzocIwxjaxDhw6MGTOGE088kTZt2pCUlHRk3/HWgqiviIgIZsyYwaWXXnqkk/rGG29k3759TJ48meLiYlSVP//5zwDcc889bNy4EVVlwoQJDBo0qEHisPUganDPO6v4ZO0u0h+YSGiwVbiMaUy2HkTDsvUgGtiE1CQOFJexLHNfoEMxxphGZQmiBmP7xBMWEmSjqo0xDeaWW25h8ODBxzxmzJgR6LB+wvogahAVHsKYXh2Yv243vz03FXedbGOMqbNnn3020CH4xGoQPkjrn8TWvQfZlFMY6FCMMabRWILwwYR+zp0L82xUtTGmFbEE4YOOsRGc1CWW+RmWIIwxrYclCB+lpSbxXVYeewoPBzoUY4xpFJYgfJTWPxFV+Gy93c1kjPEuOjoagB07djBlyhSvx5x++ukcb7xWcnIye/bs8Ut8tWUJwkf9O7Wlc2yENTMZY2rUuXPnJrXwT13Zba4+EhHS+ifxzvJsikvLiQgNDnRIxrQu/70Pdn3fsOfseBKc/ftqd99777306NGDm2++GYCHHnoIEWHRokXs37+f0tJSHn30USZPnnxMuczMTM477zzWrFnDoUOHmD59OhkZGaSmpnLo0CGfw/vTn/7Eyy+/DMD111/PHXfcQVFREZdddhnZ2dmUl5fzwAMPcPnll3tdJ6K+LEHUQlpqEq8u2crizXs4o19SzQWMMc3a1KlTueOOO44kiLfffpuPP/6YO++8k7Zt27Jnzx5OPvlkLrjggmrHSD3//PNERkayevVqVq9ezdChQ326dnp6OjNmzODbb79FVRk1ahTjxo1jy5YtdO7cmQ8//BBwJg2sXCdi/fr1iEidljv1xhJELYzq2Z6osGDmZeRYgjCmsR3nl76/DBkyhJycHHbs2EFubi7t2rWjU6dO3HnnnSxatIigoCC2b9/O7t276dixo9dzLFq0iNtvvx2AgQMHMnDgQJ+u/dVXX3HRRRcdmWL84osv5ssvv2TSpEncfffd3HvvvZx33nmMHTuWsrIyr+tE1Jf1QdRCeEgw4/om8Nn63VRUtJxJDo0x1ZsyZQozZ87krbfeYurUqbz22mvk5uaSnp7OypUrSUpK8roOhKe6zMBQ3USqJ5xwAunp6Zx00kncf//9PPLII9WuE1Fffk0QIjJJRDaIyCYRuc/L/tNFJF9EVrqP3/laNlDSUpPYfeAwa3bkBzoUY0wjmDp1Km+++SYzZ85kypQp5Ofnk5iYSGhoKAsXLmTr1q3HLX/aaafx2muvAbBmzRpWr17t03VPO+00Zs+ezcGDBykqKmLWrFmMHTuWHTt2EBkZyVVXXcXdd9/NihUrql0nor781sQkIsHAs8BEIBtYJiJzVTWjyqFfqup5dSzb6Mb3TSRIYH7GbgZ2jQt0OMYYPxswYAAFBQV06dKFTp06ceWVV3L++eczfPhwBg8eTL9+/Y5b/qabbmL69OkMHDiQwYMHM3LkSJ+uO3ToUKZNm3bk+Ouvv54hQ4bwySefcM899xAUFERoaCjPP/88BQUFXteJqC+/rQchIqOBh1T1LPf1/QCq+rjHMacDd3tJEDWW9cYf60F4c9mLSygoLuO/vxrr92sZ05rZehANqymtB9EF8Fz3LtvdVtVoEVklIv8VkcpVyX0ti4jcICLLRWR5bm7jLA2alprIup0HyN5/sFGuZ4wxgeDPBOGtV6ZqdWUF0ENVBwF/BWbXoqyzUfUlVR2uqsMTEhLqGmutpKU6dzDZGhHGmLoaNWrUT9aE+P77Bh7nUU/+vM01G+jm8borsMPzAFU94PH8IxF5TkTifSkbSD0ToumZEMX8dbu59pTkQIdjTIumqi1yHZZvv/22Ua9Xl+4Ef9YglgF9RCRFRMKAqcBczwNEpKO4/+VFZKQbz15fygbaxNQkvtmyl4Li0kCHYkyLFRERwd69e+v05WaOUlX27t1LRERErcr5rQahqmUicivwCRAMvKyqa0XkRnf/C8AU4CYRKQMOAVPV+T/Ba1l/xVoXaf2TeHHRFhb9sIdzB3YKdDjGtEhdu3YlOzubxupfbMkiIiLo2rVrrcr47S6mQGisu5gAyiuU4Y/O4/S+ifz58sGNck1jjGlogbqLqUULDhLG90vks/U5lJVXBDocY4xpcJYg6mFiahL5h0pJ37o/0KEYY0yDswRRD2NPSCAsOIj5tla1MaYFsgRRD9HhIYzu1YF5GbvtLgtjTItjCaKe0vonkbn3IJtziwIdijHGNChLEPU0oV8igDUzGWNaHEsQ9dQ5rg0DOrdlgSUIY0wLYwmiAaSlJpG+dT97Cw8HOhRjjGkwliAawMT+SVQoLNxgoz2NMS1HjQlCRKJEJMh9foKIXCAiof4PrfkY0LktHdtGMD/DmpmMMS2HLzWIRUCEiHQBFgDTgX/5M6jmRkSYkJrIoo25FJeWBzocY4xpEL4kCFHVg8DFwF9V9SKgv3/Dan7S+idxsKScJVv2BjoUY4xpED4lCHcJ0CuBD91t/lxHolka3bMDkWHBdjeTMabF8CVB3AHcD8xyp+vuCSz0a1TNUERoMKf1SWB+Ro6NqjbGtAg1JghV/UJVL1DVP7id1XtU9fZGiK3ZSeufxK4DxazdcaDmg40xponz5S6m10WkrYhEARnABhG5x/+hNT/j+yYQJDDP7mYyxrQAvjQx9XfXjr4Q+AjoDlztz6Caqw7R4Qzt3s6m3TDGtAi+JIhQd9zDhcAcVS0FrJG9Gmn9k1i74wA78w8FOhRjjKkXXxLEi0AmEAUsEpEegDWyVyMtNQmA+etyAhyJMcbUjy+d1M+oahdVPUcdW4HxjRBbs9QrIYqU+CgbVW2MafZ86aSOFZE/ichy9/EUTm3CeCEipKUmsmTzXgoPlwU6HGOMqTNfmpheBgqAy9zHAWCGP4Nq7iakJlFSXsGXP9jkfcaY5suXBNFLVR9U1S3u42Ggp78Da86G92hHbJtQ5tndTMaYZsyXBHFIRE6tfCEiYwC7Rec4QoKDOKNfIgvX51BeYTd8GWOaJ18SxE3AsyKSKSJbgb8BN/o3rOYvLTWJ/QdLWbFtf6BDMcaYOqlx0j1VXQkMEpG27mu7xdUHp50QT2iwMD9jNyOS2wc6HGOMqbVqE4SI/E812wFQ1T/5KaYWISYilJN7dmDeut3cf05qoMMxxphaO14TU0wND1ODtNQktuQWsSW3MNChGGNMrVVbg3DvVjL1MCE1kQfnrmXBuhx6JkQHOhxjjKkVXzqp60xEJonIBhHZJCL3Hee4ESJSLiJTPLZlisj3IrJSRJb7M052rYHChh+z0LVdJKmd2trtrsaYZslvCUJEgoFngbNxlii9QkR+slSpe9wfgE+8nGa8qg5W1eH+ipOD++CfZ8J/f+2X009MTWR55j72F5X45fzGGOMvvky1EVzHc48ENrmD60qAN4HJXo67DXgXCMzsdpHt4dQ7YO17sO6DBj99Wv8kKhQWbrDJ+4wxzYsvNYhNIvKEt1//NegCZHm8zna3HSEiXYCLgBe8lFfgUxFJF5EbqruIiNxQOU9Ubm4dm4lOvROSToIP/wcONey4hRM7x5IYE25rRBhjmh1fEsRA4AfgHyLyjfuF3NaHcuJlW9VhxU8D96pquZdjx6jqUJwmqltE5DRvF1HVl1R1uKoOT0hI8CEsL4JDYfLfoGgPfPLbup2jGkFBwoTUJL7YkMvhMm9v0xhjmiZfpvsuUNW/q+opwK+BB4GdIvKKiPQ+TtFsoJvH667AjirHDAfeFJFMYArwnIhc6F53h/s3B5iF02TlP50Hw5jbYeV/YNOCBj31xP6JFJWU8+2WfQ16XmOM8Sef+iBE5AIRmQX8BXgKZ7K+93GWIK3OMqCPiKSISBgwFZjreYCqpqhqsqomAzOBm1V1tohEiUiMe/0o4ExgTe3fXi2Nuw869IH374DDDTd24ZRe8bQJDbZmJmNMs+JLE9NGnM7lJ1R1iKr+SVV3q+pM4OPqCqlqGXArzt1J64C3VXWtiNwoIjXN5ZQEfCUiq4ClwIeqWu21GkxohNPUlJ8FCxpuGEhEaDBj+8QzP2M3qjZ5nzGmeahxLiZgoKp6/Tmtqrcfr6CqfkSVWoaqeuuQRlWneTzfAgzyIbaG1/1kGPVL+PYFGHAR9DilQU6blprEpxm7ydh5gAGdYxvknMYY40++1CASReR9EdkjIjkiMkdEWvZ6EGc8AHHdYc6tUNowM5uP75eICMzPsNtdjTHNgy8J4nXgbaAj0Bl4B3jDn0EFXHg0nP8M7NsMnz/eIKdMiAlnSLc4Fqy3fghjTPPgS4IQVf23qpa5j//w09tVW55e42HI1bD4r7A9vUFOmdY/idXZ+ezKL26Q8xljjD/5kiAWish9IpIsIj1E5NfAhyLSXkRa9kIHZz4K0Ukw5zYoq/9UGRNTkwCsFmGMaRZ8SRCXA78EFgKf46wwdx2QDvh3Er1AaxMH5/0ZctbCV/Vf/qJ3YjTd20cyP8MShDGm6fNlRbmUxgikyep7Npw4BRY9CakXQFJtZxw5SkRIS03iP99u5WBJGZFhvtxEZowxgeHLQLlQEbldRGa6j1tFJLQxgmsyzv4DRLSFObdAeVm9TpXWP5GSsgq+3LingYIzxhj/8KWJ6XlgGPCc+xjmbms9ouLhnCdgxwr45rl6nWpEcnvaRoRYM5MxpsnzpY1jhKp6Dlr7zB3h3LoMuBi+fxcWPgZ9z4H4401DVb3Q4CDG90vks/U5lFcowUHe5jQ0xpjA86UGUS4ivSpfuIPkWt+0pCJw7lMQHA5zb4OKijqfKi01ib1FJazMatipxY0xpiH5kiDuxrnV9XMR+QL4DLjLv2E1UW07wVmPwbbFsPyfdT7NuL4JhAQJ82xUtTGmCTtugnBXkxsE9AFudx99VXVhI8TWNA25CnqOh/kPQd62Op2ibUQoo3q2t9ldjTFN2nEThLuQzwWqelhVV6vqKlU93EixNU0icP5fQBXe/5Xztw7SUpPYlFNI5p6iBg7QGGMahi9NTItF5G8iMlZEhlY+/B5ZU9auB6Q9BJs/g5Wv1+kUae6oaqtFGGOaKl8SxCnAAOARnMWCngKe9GdQzcKI66H7aPjkfijYVevi3dpH0q9jjCUIY0yT5UuC+Lmqjvd8ANf7O7AmLygILvgrlBbDh3fVqalpQmoiyzL3k3ew/vM8GWNMQ/MlQcz0su2dhg6kWYrvA+N/A+s/gIzZtS6elppEeYXy+Ybcho/NGGPqqdoEISL9ROQSIFZELvZ4TAMiGi3Cpm70rdBpMHx4NxTtrVXRQV3jiI8Ot2YmY0yTdLwaRF/gPCAOON/jMRT4hd8jay6CQ2Dys1CcBx/fV6uiQUFCWmoiX2zIpaSs7gPvjDHGH6qdakNV5wBzRGS0qi5pxJian44nwti74Is/wElT4ISzfC6alprEm8uyWPrjPk7tE+/HII0xpnZ86YPYJCK/EZGXROTlyoffI2tuxt4Nif3h/TugON/nYmN6xxMRGmTNTMaYJseXBDEHiAXmAx96PIynkDCY/Dco3AWfPuBzsTZhwZzaO555GbvROg66M8YYf/BlNtdIVb3X75G0BF2GwehbnHWsT7wEeo7zqVhaahLz1+WwflcBqZ3a+jlIY4zxjS81iA9E5By/R9JSnP4baN/TmfG1xLdpNM5ITQRggTUzGWOaEF8SxK9wkkSxiBwQkQIROeDvwJqtsEi44G+QtxU+e9SnIokxEQzuFse8dTa7qzGm6agxQahqjKoGqWqEqrZ1X1s7yPEkj3Gm4vjmecha6lORif2TWJWVR86BYj8HZ4wxvvFlTWoRkatE5AH3dTcRGen/0Jq5tIcgtquzjnVpzV/6E/s7k/c98ckG66w2xjQJvjQxPQeMBn7mvi4EnvVbRC1FeAyc/zTs+QEW/bHGw09IiuH2CX14Jz2bP8/f6P/4jDGmBr4kiFGqegtQDKCq+4EwX04uIpNEZIOIbBKRaocZi8gIESkXkSm1Lduk9U6DQT+Dr56GnTUv431nWh8uG96VZxZs5PVv67YYkTHGNBRfEkSpu7KcAohIAlDjvBBumWeBs4H+wBUi0r+a4/4AfFLbss3CWY9BVLzT1FReetxDRYTHLjqJ8X0T+O3s75mXYXc1GWMCx5cE8QwwC0gUkceAr4D/86HcSGCTqm5R1RLgTWCyl+NuA94FcupQtumLbA/nPgW7voevn67x8NDgIJ69cigndYnltjdWkL51v/9jNMYYL3y5i+k14NfA48BO4EJV9WW67y5AlsfrbHfbESLSBbgIeKG2ZT3OcYOILBeR5bm5TXTa7NTzof+F8MUfIWd9jYdHhoXwz2kjSGobwfWvLGNzbqH/YzTGmCp8qUGgqutV9VlV/ZuqrvPx3OLtVFVePw3c6659XduylbG9pKrDVXV4QkKCj6EFwDlPQFgUzL0VKqq+3Z+Kjw7n1etGEiTCtS8vJafAbn81xjQunxJEHWUD3TxedwV2VDlmOPCmiGQCU4DnRORCH8s2L9GJMOkPkL0Mvn3RpyI9OkQxY/oI9hWVMH3GMgoPl/k5SGOMOcqfCWIZ0EdEUkQkDJgKzPU8QFVTVDVZVZNxVq67WVVn+1K2WRp4GfQ5CxY8Avu2+FakaxzPXjmU9bsKuOk/6bZuhDGm0fgyUC5KRILc5yeIyAUiElpTOVUtA27FuTtpHfC2qq4VkRtF5Ma6lK357TRxInDenyE4FObe7vM61uP7JvL4xSfx5cY93PfuahtIZ4xpFFLTl42IpANjgXbAN8By4KCqXun/8Gpn+PDhunz58kCHUbPlM+CDO+C8p2H4dJ+L/XXBRp6a9wM3nd6Leyf181t4xpjWQ0TSVXW4t32+NDGJqh4ELgb+qqoX4YxNMHU1bBokj3XWjcjf7nOxW8/ozc9Gdef5zzfzyuJMv4VnjDHgY4IQkdHAlRxdKMiXdSRMdUTggmdAy52ahI9NRiLCIxcMIC01iYfeX8vHa3b6N05jTKvmS4K4A7gfmOX2IfQEFvo1qtagfU844wHY+CmsftvnYiHBQfz1iiEM6RbH7W+uZFnmPj8GaYxpzXwZKPeFql6gqn9wO6v3qOrtjRBbyzfql9B1JHx8LxT6vhZEm7Bg/nntCLq2a8P1ryxn4+4CPwZpjGmtfLmL6XURaSsiUUAGsEFE7vF/aK1AULCzjnVJEXxUu4+0XVQYr0wfSVhIENe+vJRd+TaQzhjTsHxpYuqvqgeAC4GPgO7A1f4MqlVJ6Avjfg0Zs2Hd+7Uq2q19JDOmjSD/UCnTZizlQPHxJwM0xpja8CVBhLrjHi4E5qhqKdVMe2HqaMwd0PEk+OB/4GDt+hRO7BLLC1cPY1NOIb98NZ3DZTVP42GMMb7wJUG8CGQCUcAiEekB2JrUDSk4FCY/C4f2w+uXw+HaTc43tk8CT1w6kCVb9nL3O6upqLD8bYypP186qZ9R1S6qeo46tgLjGyG21qXTIJjyMmxPhzemQumhWhW/aEhX7ju7H++v2sHj//V1PkVjjKmeL53UsSLyp8optUXkKZzahGlo/S+Ai16AzK/grauh7HCtiv/ytJ5MOyWZv3/5I//40re5nowxpjq+NDG9DBQAl7mPA8AMfwbVqg28zFnLetM8ePfnUO77DK4iwgPn9efsEzvy6IfreH9V854A1xgTWL6MiO6lqpd4vH5YRFb6KR4DzlQcpYfg4/tg9k1OrSIo2KeiwUHCny8fzN7Cpdz19irio8MZ3auDf+M1xrRIvtQgDonIqZUvRGQMULsGclN7J9/kjLT+/u1aTccBEBEazN+vGU6PDpHc8O/lrN9l9xQYY2rPlwRxI/CsiGS6C/v8DfilX6MyjtPuhrF3wYpXndpELZJEbGQo/7puJFFhIVz78lK251lON8bUznEThIgEA1ep6iBgIDBQVYeo6upGic44tYhRN8G3LzgLDdVCl7g2/Ou6ERw8XM60l5eSf9AG0hljfHfcBOGuFT3MfX7AHVFtGpMITHrc6Zf46k+w6IlaFe/XsS0vXjOMrXsP8otXl1NcagPpjDG+8aWJ6TsRmSsiV4vIxZUPv0dmjhKBc/8MAy+Hzx6FJc/VqvgpveJ56rJBLM3cx/+8vZJyG0hnjPGBL3cxtQf2Amd4bFPgPb9EZLwLCoLJzzl3N31yP4S2qdVqdOcP6szuA8U8+uE6EmMyePD8/oiIHwM2xjR3NSYIVfX9W8j4V3AIXPJPeOtK+OBOCI2EQZf7XPz6sT3ZlV/MP776kU6xEfxyXC8/BmuMae58GUn9iojEebxuJyIv+zUqU72QMLjsVUgZC7NvhIw5tSr+m3NSOX9QZx7/73pmfZftpyCNMS2BL30QA1U1r/KFqu4HhvgtIlOz0DYw9Q3oOgJm/hx++NTnokFBwpOXDmR0zw7c885qvtyY68dAjTHNmS8JIkhE2lW+EJH22JrUgRceDVe+A0n94a2rYMsXvhcNCebFa4bROzGaG/+dzprt+X4M1BjTXPmSIJ4CFovI/4rII8Bi4I/+Dcv4JCIWrprlrG/9xhWw7Rufi7aNCOVf00cS2yaU6f9aRta+g34M1BjTHPky3ferwCXAbiAXuFhV/+3vwIyPojrANXMgpiO8dilsX+Fz0Y6xEbxy3UgOl5Zz7Yyl7C8q8WOgxpjmxpcaBKqaoap/U9W/qmqGv4MytRSTBNfOhYg4+M/FsHutz0X7JMXwj2tHkL3/ED9/ZZkNpDPGHOFTgjDNQGxXuHYOhETAqxfCnk0+Fx2Z0p5npg7mu6w8bnvjOxtIZ4wBLEG0LO17wjVzQSvg1Qtg/1afi046sRMPnT+AeRm7eWDOGkrLK/wYqDGmObAE0dIknADXzIaSInjlfDjg+6JB156SzE2n9+L1b7cx9g8LeXbhJuuXMKYVE63FFNJN3fDhw3X58uWBDqNpyE6HVydD204w7SOITvCpmKqycEMOL3+VyVeb9hAeEsTFQ7sw7ZQU+naM8XPQxpjGJiLpqjrc6z5/JggRmQT8BQgG/qGqv6+yfzLwv0AFUAbcoapfufsycZY6LQfKqnsDnixBVLF1Mfz7YujQC659HyLb16r4D7sLmPF1Ju+tyOZwWQVjenfgujEpjO+bSFCQzeNkTEsQkAThriXxAzARyAaWAVd43gUlItFAkaqqiAwE3lbVfu6+TGC4qu7x9ZqWILzY/Bm8fjkknejcDhvRttan2F9UwhvLtvHvJVvZmV9McodIrj0lmUuHdyM63MZMGtOcHS9B+LMPYiSwSVW3qGoJ8CYw2fMAVS3UoxkqCmeWWNOQep0Bl74Cu1Y7iaKkqNanaBcVxs2n92bRr8fz1yuG0D4qjIffz+Dk/1vAI+9nsG2vDbIzpiXyZ4LoAmR5vM52tx1DRC4SkfXAh8B1HrsU+FRE0kXkhuouIiI3iMhyEVmem2vzCnnV7xy4+CXI+gbevBJKi+t0mtDgIM4f1Jn3bh7D7FvGMCE1kVeXZDLuyYX84tXlLN68h5bUp2VMa+fPJqZLgbNU9Xr39dXASFW9rZrjTwN+p6pp7uvOqrpDRBKBecBtqrroeNe0JqYafPcazLkZTjgbLv83BIfW+5S7DxTzn2+28tq329hXVEK/jjFcNyaFCwZ3JiI0uAGCNsb4U6CamLKBbh6vuwLV3nPpfvn3EpF49/UO928OMAunycrUx5Ar4Zwn4Yf/wnu/gIr6j5pOahvBXWf2ZfF9Z/DHSwYC8Ot3V3PK7z/jyU82sPtA3WorxpjA82cP4zKgj4ikANuBqcDPPA8Qkd7AZreTeigQBuwVkSggSFUL3OdnAo/4MdbWY+QvnFXp5j0AIW1g8rPOanX1FBEazGUjunHp8K4s2bKXGV9n8uznm3jhi82cO7AT08ekMLhbXP3jN8Y0Gr8lCFUtE5FbgU9wbnN9WVXXisiN7v4XcCYBvEZESoFDwOVuskgCZrlLYoYAr6vqx/6KtdUZczuUHoTPH3fWljj3KWfd6wYgIpzSK55TesWzbe9BXlmSydvLspizcgdDu8cxfUwKk07sSGiwjdE0pqmzgXKtlSrM+x0sfgZOuQ0m/m+DJYmqCg+XMXN5Fv9anEnm3oN0io3g6tE9uGJEd9pFhfnlmsYY3wRsoFxjswRRS6rw0T2w7O8w7j4Yf79fL1dR4YzSnvG1M0o7IjSIi4Z0YfqYFE5IslHaxgTC8RKEjXJqzUTg7D86zU1f/B7CImHMr/x2uaAgYUJqEhNSk9iwq4B/Lf6R91Zs542lWZzaO57pY5JtlLYxTYjVIIxzN9O718Pa95y7nEb+otEuXTlK+9XFW9l1oJiU+CiuHd2DKTZK25hGYU1MpmblpfD2NbDhI+fOpiFXNerlS8sr+HjNLmZ8/SMrtuUREx7C5CGdGZXSgSHd4+gS1wbxUx+JMa2ZJQjjm9JieGMqbFkInYfCiRdD/wshrluNRRvSyqw8Znz9I5+s3UVxqbMuRWJMOEO6xzG0ezuGdG/HwK6xNhDPmAZgCcL4ruQgLPsHrHkXdq50tnUdAQMuhv6TIfYns6X4TWl5Bet3FvBd1n5WbN3Pd1l5bHXnfQoJElI7tWVo9ziGdG/H0O7t6NbeahnG1JYlCFM3+7bA2lnOY9f3zrbuo2HARU6yiOnY6CHtLTzMd9vy3KSRx6rsPA6WOCPCO0SFMcRNGEO6xzGoaxxR1o9hzHFZgjD1t2fT0WSRsxYQ6DEGBlzoJIvoxICEVVZewQ+7C48kjO+y9rMl15mxNkigb0fPWkYcKfFRVsswxoMlCNOwctZDxmxY8x7s2QASBMmnOjWL1AsgKj6g4e0vKmFldh7fuc1SK7flUXC4DIC4yFCGdDvaLDWoWywxEfWftNCY5soShPEPVchZ59Ys3oO9m0CCIeU0p4O733m1XsXOHyoqlE25hU4/hts8tTGnEFVnKEifxGi389vpBO+VEG1jMUyrYQnC+J8q7F7j1CrWzoL9P0JQCPQc79Qs+p0DbdoFOsoj8g+Vsjo770iz1Hfb8sg/VApATEQIg7sdbZY6sUssHaLCrGnKtEiWIEzjUoWdq5xaxdpZkLcNgkKh9wQnWfQ9p05Ln/pTRYXy496iI3dLrdi6nx92F1Dh/vOIiQihZ3wUyfFRpHg8kuOjaGtNVKYZswRhAkcVtq9wk8VsOJANweHQO81NFpMgvGnOw1R4uIzVWXms31XAj3uKyNxbxJbcInbkH8Lzn018dJiTLDpEkZIQdSSRJHeIqv9YjZKDkPkV/PgFtE+BIddAiE1waBqOJQjTNFRUwPblbp/FbCjYASER0GeikyxOmARhUYGOskbFpeVs23eQLblO0vgxt4gf9xbx454icgsOH3Ns59gIUhLc5BEfRU/3ebf2kd6nPFeF3PWwaT5sWgBbF0P5YacGVlEKcT3gjN/CiVMaZB0PYyxBmKanogKyvnVqFhlzoHC3s4DRCWc5Hdy9JzqTBzYzhYfLyNzjJIuqj8o+DoDgIKFbuzakxEfRN66C0ayhb+G3xO/+ipBCd+HFhFSnWa73BOh+ilOTWPCQMyYl6USY8KCTXK1vxNSDJQjTtFWUw7YlTgd3xhw4uAdCo6D3GdBlGHQcCJ0GBfz22fraX1TClj1FZOYWcHBrOnE7FpGS/w39StcTIhUc0DZ8VXESXzOYzLhRRCcmkxzvNFmlxEfTOS6C+KhQIjbMgc/+F/ZnOmNR0h6CbrYir6kbSxCm+Sgvg61fOzWLzZ85HdyV2nZxEkVlwug00NnWHH5BF+Y672fTfOfvwT3O9s5D0F4T2N/5NH4I6cuP+0v4cU/RkearbXsPUlJeccypYtuE0ik6iMuCFjKl8DXalu/nx/jT2XTinUR0GUBCTDgJ0eG0iwyz23VNjSxBmObr0H6nSWXnKti52vm7dyOo+6XZpv3RZNFpEHQcBO17Br59vrwUspe5fQnznbgBIuPdZqM05xbg6ITjn6ZC2ZF3iC17itiVf4jcgsPOo/AwOQcOU1CQz9mFs7hO5hJFMe+Vj+XpskvYTgIhQUJ8dLiTMNykkdj26POEmHASYyJIiAmnTZhNfNhaWYIwLUtJEexe6yaNVbBrNezOcDpxAcKioeNJx9Y2EvpCsJ9vR83Lgs0LnISw5Qs4fMAZONht1NG+hI6D/JK8ivbnUPbFk8SsnoGirO96OZ8lXM224jbkFh4+klj2FB4+cuuup+jwkKOJo+3RBFL5SHT/dogKJ9hqJS2KJQjT8pWVOHf/VCaMnatg1xoodeZlIjgckvp7NE8NgqQBENqm7tcsLXaawza5SWHPBmd7264etYRxEBFb//fnq7wsZ3XAla87/ThjboeTb4bwaMCpkewrKvGohRQfk0A8H5XTk3gKEqeJq11kGLGRzt+4NqHERYbRLjKUuMjK52Huc+eYyLBgG2jYRFmCMK1TRTns3ewmjJVHm6iK85z9EgzxJxxtouo40Kl5tInzfj5VZzqRyltQM7+CskNO8kke4ySE3mnOOQP9ZZiz3unIXv8BRCXAab+GYdNqNYbiUEk5ewoPk1NQfEzi2HewhLyDpeQdLGX/keclFLmz6noTFhxEbGQocW5y8Uwece7rdm5yqdwe2ybU1vxoBJYgjKmkCvlZx/Zp7FoNBTuPHtMu2aN5arAzDqGyL6Gy07xD76MJoceYpntLbtYymP8QbP3K72MoDpeVk3+otEryKHGfl3o8LyH/kPN3/8FSSsoqqj1nm9Bg2kWGEuvWUI7WXEKJaxN2NOlEOTWZ2MhQYtuEEh5iicVXliCMqUlhjpswVh5totqfeXR/WDSkjDval9AuOUCB1oGqU+NpgmMoVJXi0go3WZSQX5lMDrnJpKiEvENOcvFMMnmHSin31pniigwLdhNGZROY84htc7QpLLbN0ZpMnPu8NdZYLEEYUxeH8pwvVBHoOrL5T3FRUeHcPlw5hqL7Kc4Yiu6jAh1ZrakqhYfLyDtYeqQ2Upk48j2eO/s9X5dQWl79d154SNAxCcPzeazndre20i4yjPZRYc06sViCMMYcVVYCK16BL/4IRTnO5IkTfgeJqYGOzO9UlYMl5UeSRb5HIsk75L6ubB47VOru960prH1UGO2ijiaNI3+jwmgf6exr7z6PiwwjLKRpTJViCcIY81MlRfDNc/D1M3C4AAZdAePvh7jugY6sSSouLT+SSDz7WfYfLGF/UQn7ipzX+4pKjvwtKP7pnWCVYsJDaHckgYR6JJJjE0z7qKOd+f64xdgShDGmegf3wZdPwdK/Awojroexd0NUh0BH1uyVlFU4NZCi0mMSx/6iEvZVJha3r6Vy/8Fq7gYT9xbjyiTiJBAnsSTGRPDzU1PqFKMlCGNMzfKz4fPHj46hOOU2GH3LkTEUpnEUl5Z7JJLSo4nEM8EcLGFvYWXtpZT2UWF885sJdbqeJQhjjO9yN8CCR5wxFJHxMO7XMGx68++kb6Eq7wSr63QpAUsQIjIJ+AsQDPxDVX9fZf9k4H+BCqAMuENVv/KlrDeWIIxpQNnLnTEUmV86/RLjfwsnXRr4ea7qSxXKDkNJofM4XOj0x5QUOH8PF3rZ5/na3Xa4wGn36TIMepzi3BWW0K/ZfT4BSRAiEgz8AEwEsoFlwBWqmuFxTDRQpKoqIgOBt1W1ny9lvbEEYUwDU3Xml5r/kHPLb+IA6DIUgoJBgpzR6EHB7l/3tQR5bPN87e34qtsqj6t6jirHi0DpIedLuvIL3POLvdovevf4iuo7j48RFOKMgQmPcRazCot2/obHOM/LiiFrqbP4FUBEHHQfDT1GOwmj06AmX/M6XoII8eN1RwKbVHWLG8SbwGTgyJe8qhZ6HB8FqK9ljTGNQMSdU+oMZwzF1087I8orykHL3b/q8bzybwVH/zk3opAI9ws92v0yj3a+tGO7Qpj7JR/ufsmHxXg8j/5pufBoCA6reTChKuRtdVb/27rYWdvkh/+68bSBrsPdGsZoZ92OZrBqYiV/JoguQJbH62zgJyNyROQi4HEgETi3NmXd8jcANwB072635xnjF0FBcNIU5+Er1Z8mDc/nlfs8n1dUeNlWWbZKmdAojy9090s+2J9fadUQcUbWt0uGwT9zthXmOIli6xLYthgWPeHELMFOraLHKUeTRmT7xo/ZR/78NL2l3Z/8pFDVWcAsETkNpz8izdeybvmXgJfAaWKqc7TGmIYl4n5hB+BLO9CiE6H/ZOcBUHzAaYratthJGkv/Dkv+5uxL6Oc2S7kJI65b4OKuwp//5bIBz3faFdhR3cGqukhEeolIfG3LGmNMkxbRFvqkOQ9wOsm3rziaMNa8C+kznH2x3Y7tx0joG7A5s/yZIJYBfUQkBdgOTAV+5nmAiPQGNrud1EOBMGAvkFdTWWOMabZCwp0E0GM0jMVpOtu91u3DWAxbPofv33aObdP+aMLocYqz6FQjNaX57SqqWiYitwKf4Nyq+rKqrhWRG939LwCXANeISClwCLhcnduqvJb1V6zGGBNQQcHusrkD4eQbnf6bfVuOdnpvXQwbPnSODY2CbiOc2kWP0dBluN+mm7eBcsYY0xwc2Hk0WWxb4tQ4UAgKha4jYNqHdRqDEajbXI0xxjSUtp3gxIudBzjT0Wd96ySMQ/v9MkDPEoQxxjRHbeLghLOch580rzHhxhhjGo0lCGOMMV5ZgjDGGOOVJQhjjDFeWYIwxhjjlSUIY4wxXlmCMMYY45UlCGOMMV61qKk2RCQX2BroOOopHtgT6CCaCPssjmWfx7Hs8ziqPp9FD1VN8LajRSWIlkBEllc3L0prY5/FsezzOJZ9Hkf567OwJiZjjDFeWYIwxhjjlSWIpuelQAfQhNhncSz7PI5ln8dRfvksrA/CGGOMV1aDMMYY45UlCGOMMV5ZgmgCRKSbiCwUkXUislZEfhXomAJNRIJF5DsR+SDQsQSaiMSJyEwRWe/+PzI60DEFkojc6f47WSMib4hIRKBjakwi8rKI5IjIGo9t7UVknohsdP+2a4hrWYJoGsqAu1Q1FTgZuEVE+gc4pkD7FbAu0EE0EX8BPlbVfsAgWvHnIiJdgNuB4ap6IhAMTA1sVI3uX8CkKtvuAxaoah9ggfu63ixBNAGqulNVV7jPC3C+ALoENqrAEZGuwLnAPwIdS6CJSFvgNOCfAKpaoqp5AQ0q8EKANiISAkQCOwIcT6NS1UXAviqbJwOvuM9fAS5siGtZgmhiRCQZGAJ8G+BQAulp4NdARYDjaAp6ArnADLfJ7R8iEhXooAJFVbcDTwLbgJ1Avqp+GtiomoQkVd0Jzg9OILEhTmoJogkRkWjgXeAOVT0Q6HgCQUTOA3JUNT3QsTQRIcBQ4HlVHQIU0UDNB82R27Y+GUgBOgNRInJVYKNquSxBNBEiEoqTHF5T1fcCHU8AjQEuEJFM4E3gDBH5T2BDCqhsIFtVK2uUM3ESRmuVBvyoqrmqWgq8B5wS4Jiagt0i0gnA/ZvTECe1BNEEiIjgtDGvU9U/BTqeQFLV+1W1q6om43Q+fqaqrfYXoqruArJEpK+7aQKQEcCQAm0bcLKIRLr/bibQijvtPcwFrnWfXwvMaYiThjTESUy9jQGuBr4XkZXutt+o6keBC8k0IbcBr4lIGLAFmB7geAJGVb8VkZnACpy7/76jlU25ISJvAKcD8SKSDTwI/B54W0R+jpNEL22Qa9lUG8YYY7yxJiZjjDFeWYIwxhjjlSUIY4wxXlmCMMYY45UlCGOMMV5ZgjCmCRCR023mWtPUWIIwxhjjlSUIY2pBRK4SkaUislJEXnTXrSgUkadEZIWILBCRBPfYwSLyjYisFpFZlXP0i0hvEZkvIqvcMr3c00d7rPvwmjtS2JiAsQRhjI9EJBW4HBijqoOBcuBKIApYoapDgS9wRrYCvArcq6oDge89tr8GPKuqg3DmEdrpbh8C3AH0x5nFdYyf35Ixx2VTbRjjuwnAMGCZ++O+Dc6kaBXAW+4x/wHeE5FYIE5Vv3C3vwK8IyIxQBdVnQWgqsUA7vmWqmq2+3olkAx85fd3ZUw1LEEY4zsBXlHV+4/ZKPJAleOON3/N8ZqNDns8L8f+fZoAsyYmY3y3AJgiIolwZB3gHjj/jqa4x/wM+EpV84H9IjLW3X418IW7zke2iFzoniNcRCIb800Y4yv7hWKMj1Q1Q0R+C3wqIkFAKXALziI+A0QkHcjH6acAZ9rlF9wE4DkL69XAiyLyiHuOBpl505iGZrO5GlNPIlKoqtGBjsOYhmZNTMYYY7yyGoQxxhivrAZhjDHGK0sQxhhjvLIEYYwxxitLEMYYY7yyBGGMMcar/w80iBvfH7l/VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_plot = pd.DataFrame()\n",
    "df_plot['train_loss'] = train_losses\n",
    "df_plot['valid_loss'] = valid_losses\n",
    "df_plot['epoch'] = range(1, epochs+1)\n",
    "df_plot.to_csv('results/bert_v1.csv')\n",
    "\n",
    "sns.lineplot(x='epoch', \n",
    "             y='value', \n",
    "             data=pd.melt(df_plot.reset_index(), \n",
    "                          id_vars='epoch', \n",
    "                          value_vars=['train_loss', 'valid_loss']),\n",
    "             hue='variable')\n",
    "plt.ylabel('cross entropy loss')\n",
    "plt.title('BERT, frozen parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1e3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
