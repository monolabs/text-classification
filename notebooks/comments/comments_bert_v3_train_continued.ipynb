{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249dac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd() + '/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e7cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scripts.bert_utils import *\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4a7cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d'aww ! he match this background colour i be s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man , i be really not try to edit war . it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>`` more i can not make any real suggestion on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you , sir , be my hero . any chance you rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                       comment_text\n",
       "0      0  explanation why the edits make under my userna...\n",
       "1      0  d'aww ! he match this background colour i be s...\n",
       "2      0  hey man , i be really not try to edit war . it...\n",
       "3      0  `` more i can not make any real suggestion on ...\n",
       "4      0  you , sir , be my hero . any chance you rememb..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('data/comments/preprocessed_comments.csv', index_col=0)\n",
    "comments = comments.dropna()\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb241615",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text, test_text, temp_labels, test_labels = train_test_split(comments['comment_text'], \n",
    "                                                                  comments['toxic'], \n",
    "                                                                  random_state=0, \n",
    "                                                                  test_size=0.2, \n",
    "                                                                  stratify=comments['toxic'])\n",
    "\n",
    "\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(temp_text, \n",
    "                                                                  temp_labels, \n",
    "                                                                  random_state=0, \n",
    "                                                                  test_size=0.2, \n",
    "                                                                  stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e82a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1886b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1000.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "plt.hist(seq_len, bins=100)\n",
    "plt.vlines(256, 0, 60000, linestyles='dotted', color='k')\n",
    "plt.xlim(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e011cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 128\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = n_tokens,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = n_tokens,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = n_tokens,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb1d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a107d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b072dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b986b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b97e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06371134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'models/bert_v3.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58589896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-7)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59f308b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.55300208 5.21679608]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', y=train_labels, classes=np.unique(train_labels))\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4141d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb7e8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(model, train_dataloader, loss_function):\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_function(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b5f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "            \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "      \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47b4863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch   100  of  6,383.\n",
      "  Batch   200  of  6,383.\n",
      "  Batch   300  of  6,383.\n",
      "  Batch   400  of  6,383.\n",
      "  Batch   500  of  6,383.\n",
      "  Batch   600  of  6,383.\n",
      "  Batch   700  of  6,383.\n",
      "  Batch   800  of  6,383.\n",
      "  Batch   900  of  6,383.\n",
      "  Batch 1,000  of  6,383.\n",
      "  Batch 1,100  of  6,383.\n",
      "  Batch 1,200  of  6,383.\n",
      "  Batch 1,300  of  6,383.\n",
      "  Batch 1,400  of  6,383.\n",
      "  Batch 1,500  of  6,383.\n",
      "  Batch 1,600  of  6,383.\n",
      "  Batch 1,700  of  6,383.\n",
      "  Batch 1,800  of  6,383.\n",
      "  Batch 1,900  of  6,383.\n",
      "  Batch 2,000  of  6,383.\n",
      "  Batch 2,100  of  6,383.\n",
      "  Batch 2,200  of  6,383.\n",
      "  Batch 2,300  of  6,383.\n",
      "  Batch 2,400  of  6,383.\n",
      "  Batch 2,500  of  6,383.\n",
      "  Batch 2,600  of  6,383.\n",
      "  Batch 2,700  of  6,383.\n",
      "  Batch 2,800  of  6,383.\n",
      "  Batch 2,900  of  6,383.\n",
      "  Batch 3,000  of  6,383.\n",
      "  Batch 3,100  of  6,383.\n",
      "  Batch 3,200  of  6,383.\n",
      "  Batch 3,300  of  6,383.\n",
      "  Batch 3,400  of  6,383.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7df6232912ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-aac7ecf1ff78>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, loss_function)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# backward pass to calculate the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train(model, train_dataloader, cross_entropy)\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/bert_v3c.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame()\n",
    "df_plot['train_loss'] = train_losses\n",
    "df_plot['valid_loss'] = valid_losses\n",
    "df_plot['epoch'] = range(1+10, epochs+1+10)\n",
    "df_plot.to_csv('results/bert_v3c.csv')\n",
    "\n",
    "sns.lineplot(x='epoch', \n",
    "             y='value', \n",
    "             data=pd.melt(df_plot.reset_index(), \n",
    "                          id_vars='epoch', \n",
    "                          value_vars=['train_loss', 'valid_loss']),\n",
    "             hue='variable')\n",
    "plt.ylabel('cross entropy loss')\n",
    "plt.title('BERT, frozen parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1e3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
